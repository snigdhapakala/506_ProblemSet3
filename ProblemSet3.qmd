---
title: "STATS 506 Problem Set #3"
author: "Snigdha Pakala"
editor: visual
format:
  html:
    embed-resources: true
---

### Link to my GitHub repository: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

## Problem 1

### 1a:

```{r}
install.packages("haven")
library(haven)
vision_data <- read_xpt("VIX_D.xpt")
demo_data <- read_xpt("DEMO_D.xpt")

# merge both datasets by SEQN variable
merged_df <- merge(demo_data, vision_data, by = "SEQN")

# Confirm row count matches 6980
nrow(merged_df)
```

### 1b:

```{r}
#Estimate proportion of age brackets with distance glasses

# First create the age brackets
library(dplyr)
merged_df$Age_Bracket <- cut(merged_df$RIDAGEYR, breaks = seq(0, max(merged_df$RIDAGEYR, na.rm = TRUE) + 10, by = 10), right = FALSE)

# Check that the ages are bucketted correctly
unique(select(merged_df, RIDAGEYR, Age_Bracket))

# distance glasses information: VIQ220 from the vision documentation

# Create contingency table; include NAs for accurate proportion calculations
contingency_table <- table(merged_df$Age_Bracket, merged_df$VIQ220, useNA = "ifany")

# Results
contingency_df <- data.frame(
  Age_Bracket = rownames(contingency_table),
  # Summed across all the rows in the contingency table to get the totals per age bracket
  Total_Pop = apply(contingency_table, 1, sum),
  # Only getting the VIQ220 == 1 population which is the first column of my table
  Glasses_Pop = contingency_table[,1],
  # Dividing the previous two columns
  Proportion = contingency_table[,1] / apply(contingency_table, 1, sum), row.names = NULL
)

# Removing Nas from showing up in nice output
contingency_df[is.na(contingency_df)] <- 0
library(kableExtra)
# Using kable to format this nicely
table <- kable(contingency_df, 
               digits = 2, 
               col.names = NULL, 
               caption = "Proportion of Individuals with Glasses by Age Bracket")
# Used the source I linked in attribution of sources for the following:
table <- kable_styling(table, 
                       bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                       full_width = F, 
                       position = "center")

# Adding the header
table <- add_header_above(table, c("Age Bracket" = 1, "Total Population" = 1, "Glasses Population" = 1, "Glasses Proportion" = 1))

# Specify column formatting for borders
table <- column_spec(table, 1, bold = TRUE, border_right = TRUE)
table <- column_spec(table, 2:4, border_left = TRUE, border_right = TRUE)

# Print the final table
table

```

### 1c:

```{r}
# Make glasses variable binary 
merged_df$Binary_Glasses <- 1 * (merged_df$VIQ220 == 1)
merged_df$Binary_Glasses[is.na(merged_df$Binary_Glasses)] <- 0

head(select(merged_df, VIQ220, Binary_Glasses))

# Fit Model 1: Age as the predictor
model1 <- glm(Binary_Glasses ~ RIDAGEYR, data = merged_df, family = binomial)

# Fit Model 2: Age, Race, and Gender as predictors
model2 <- glm(Binary_Glasses ~ RIDAGEYR + RIDRETH1 + RIAGENDR, data = merged_df, family = binomial)

# Fit Model 3: Age, Race, Gender, and Poverty Income Ratio as predictors
model3 <- glm(Binary_Glasses ~ RIDAGEYR + RIDRETH1 + RIAGENDR + INDFMPIR, data = merged_df, family = binomial)

library(pscl)

#' This function takes each of the glm models and provides all 4 statistics requested in the question
#'
#' @param model This is meant for all 3 of the glm models to be inputted
#'
#' @return a list of odds ratios, sample side, pseudo r^2, and aic values for each model input
#' @export
#'
#' @examples
logistic_reg_function <- function(model) {
  
  odds_ratios <- exp(coef(model))  
  sample_size <- nobs(model)  
  pseudo_r2 <- pR2(model)["McFadden"]  
  aic <- AIC(model)
  
  return(list(odds_ratios = odds_ratios, sample_size = sample_size, pseudo_r2 = pseudo_r2, aic = aic))
}

# Put the output of each model going into the above function into a respective object
model1_func <- logistic_reg_function(model1)
model2_func <- logistic_reg_function(model2)
model3_func <- logistic_reg_function(model3)

# Output the results in a readable format
table_models <- data.frame(
  Metric = c("Odds Ratios", "Sample Size", "Pseudo R^2", "AIC Values"),
  Model1 = c(paste(round(model1_func$odds_ratios, 2), collapse=", "), 
             model1_func$sample_size, 
             round(model1_func$pseudo_r2, 4), 
             round(model1_func$aic, 2)),
  Model2 = c(paste(round(model2_func$odds_ratios, 2), collapse=", "), 
             model2_func$sample_size, 
             round(model2_func$pseudo_r2, 4), 
             round(model2_func$aic, 2)),
  Model3 = c(paste(round(model3_func$odds_ratios, 2), collapse=", "), 
             model3_func$sample_size, 
             round(model3_func$pseudo_r2, 4), 
             round(model3_func$aic, 2))
)

table_models

```

### 1d:

```{r}
# Check odds ratio between men and women

# Null Hypothesis: the odds of wearing glasses/contacts for men and women are the same (odds ratio is 1)

# Alternate Hypothesis: the odds of wearing glasses/contacts for men and women are not the same (odds ratio is not 1)

summary(model3)

# Making female participants the reference category in regression
merged_df$RIAGENDR <- relevel(factor(merged_df$RIAGENDR), ref = "2")

confint_gender <- exp(confint(model3)["RIAGENDR", ])

# Sanity check of confidence interval
odds_ratio_gender <- exp(coef(model3)["RIAGENDR"])

# Output the confidence interval for gender (Men vs. Women)
print(paste("95% Confidence Interval for Gender Odds Ratio (Men vs. Women):", 
            round(confint_gender[1], 2), "to", round(confint_gender[2], 2)))

```

Since our 95% confidence interval does not include 1 (which is have been the case under the null), we have sufficient evidence of a difference in odds of wearing glasses/contacts between men and women. This implies that men are between 1.43 to 1.76 times more likely to be wearing glasses/contacts than women.

```{r}
# Check proportion of glasses/contact wearers between men and women

# Create a contingency table for gender and glasses wearers
table_gender_glasses <- table(merged_df$Binary_Glasses, merged_df$RIAGENDR)
print(table_gender_glasses)

# Perform chi-square test
chi_square_test <- chisq.test(table_gender_glasses)

# Output the p-value from the chi-square test
chi_square_test$p.value

print(paste("Since our p-value in our chi squared test came out to", chi_square_test$p.value, "we see evidence that the proportion of glasses/contact lens wearers differs significantly between men and women , as this value is lower than our significance level of alpha = 0.05"))

```

## Problem 2

### 2a:

```{r}
# Establish connection
library(DBI)
sakila <- dbConnect(RSQLite::SQLite(), "sakila_master.db")
dbListTables(sakila)

# Get the year of the oldest movie and the number of movies released that year 
gg <- function(query){
  dbGetQuery(sakila, query)
}

# Quick check of the data, see all the dates before putting min() on the year variable
gg("
  SELECT DISTINCT release_year
  FROM film
")
# This tells me we only have one release year in the film so this is automatically the oldest release year, which means all the rows in the table are the films released this year

# Answer:
# Find the distinct count of movies that were released in 2006:
gg("
  SELECT MIN(release_year) AS oldest_movie_year, COUNT(DISTINCT film_id) AS movies_released
  FROM film
")
```

Thus, it can be seen that 2006 is when the oldest movie in the table was released, and there are 1,000 movies in our table that were released that year.

### 2b:

```{r}
# What genre of movie is the least common in the data, and how many movies are of this genre?

# First use SQL query or queries to extract the appropriate table(s), then use regular R operations on those data.frames to answer the question. 
genre_film <- as.data.frame(
  gg("
    SELECT 
      a.category_id
      , a.name
      , b.film_id
    FROM category a
    LEFT JOIN film_category b
      ON a.category_id = b.category_id
  ")
)

# Crease a table of the genres:
table(genre_film$name)
  
# Sort in ascending order
sort(table(genre_film$name), descending = FALSE)

# Now extract the first name in the sorted table to get the genre with the least number of movies, and the respected film frequency
least_common_genre1 <- names(sort(table(genre_film$name), descending = FALSE))[1]
least_common_genre_num_films1 <- sort(table(genre_film$name), descending = FALSE)[1]

# Display results of the first method
cat(paste("Using the R method after extracting the SQL information and creating a data frame, we see that the least common genre is", least_common_genre1, "with", least_common_genre_num_films1, "films"))

###############################################################################################

# Second, use a single SQL query to answer the question.
gg("
  SELECT 
    a.name
    , COUNT(DISTINCT b.film_id) films
  FROM category a
  LEFT JOIN film_category b
    ON a.category_id = b.category_id
  GROUP BY 
    a.name
    , a.category_id
  ORDER BY films ASC
  LIMIT 1
")

```

### 2c:

```{r}
# Identify which country or countries have exactly 13 customers.

# This method requires 4 tables: Customer, Address, City, and then Country since we need the customer count which is in Customer and we need the country name which is in Country, and to connect the two we need to go through Address with address_id, City with city_id, and finally Country with country_id

# First use SQL query or queries to extract the appropriate table(s), then use regular R operations on those data.frames to answer the question. 

cust_countries <- as.data.frame(
  gg("
    SELECT 
      a.customer_id
      , b.address_id
      , c.city_id
      , d.country
    FROM customer a
    LEFT JOIN address b
      ON a.address_id = b.address_id
    LEFT JOIN city c
      ON b.city_id = c.city_id
    LEFT JOIN country d
      ON c.country_id = d.country_id
  ")
)

# Table with logical values
country_counts <- table(cust_countries$country)

# Find countries where the count equals 13
countries_13_custs <- country_counts[country_counts == 13]

# Display the result
cat(paste("Using the R method after extracting the SQL information and creating a data frame, we see that the countries with exactly 13 customers are", names(countries_13_custs)[1], "and", names(countries_13_custs)[2]))

###############################################################################################

# Second, use a single SQL query to answer the question.
gg("
  SELECT 
    e.country
    , COUNT(DISTINCT e.customer_id) AS cust_count
  FROM
      (SELECT 
        a.customer_id
        , b.address_id
        , c.city_id
        , d.country
      FROM customer a
      LEFT JOIN address b
        ON a.address_id = b.address_id
      LEFT JOIN city c
        ON b.city_id = c.city_id
      LEFT JOIN country d
        ON c.country_id = d.country_id) e
  GROUP BY e.country
  HAVING cust_count = 13
")

```

## Problem 3

### 3a:

```{r}
us500 <- read.csv("us-500.csv")
email <- us500$email
head(us500)

# Display all of the email addresses ending in .com
email[grepl("\\.com$", email)]

# Count email addresses ending in .com
dot_com_addresses <- sum(grepl("\\.com$", email))

# Total unique email addresses
total <- 500

# Display Results: Proportion of .com addresses
cat(paste("Proportion of .com addresses from this dataset is", dot_com_addresses/total))

```

### 3b:

```{r}
# [^] returns the negation of the following 
# a-zA-Z is regarding alphabetic characters both lower and upper case
# 0-9 is regarding numeric characters 
# @ and \\. exclude these from counting in the pattern
# Putting all these together returns those emails that include a character other than the ones listed here, so some non-alphanumeric character(s) excluding @ and . in the address.

# Sanity Check
head(email[grep("[^a-zA-Z0-9@\\.]", email)]) 

non_alpha_num <- sum(grepl("[^a-zA-Z0-9@\\.]", email))

# Display Results: Proportion of non-alpha-num email addresses
cat(paste("Proportion of email addresses from this dataset with an alphanumeric character besides @ and . is", non_alpha_num/total))
```

### 3c:

```{r}
# Use substr method from class/ professor's notes
area_code <- table(substr(us500$phone1, 1, 3)) 

# Extract the top 5 names and values from this table sorted in descending order
max_area_code <- sort(area_code, decreasing = TRUE)[1:5] 
max_area_code

# Extract the top 5 most common area codes by grabbing the names of these high frequency values
names(max_area_code)

# Display Results
cat(paste("The top 5 most common area codes amongst all phone numbers are:\n"))
cat(paste(names(max_area_code)), sep = "\n")
```

### 3d:

```{r}
address <- us500$address

# Get all the addresses that end with a number 
(address[grepl("[0-9]+$", address)])

# Use lecture and professor notes for the rest
table(grepl("[0-9]+$", address))

matches <- regexpr("[0-9]+$", address)

apt_nums <- as.numeric(regmatches(address, matches))

# Display results in a histogram
hist(log(apt_nums), main = "Histogram of Log of Apartment Numbers", xlab = "Log of Apartment Numbers")
```

### 3e:

```{r}
# Extract all the unique values in our list of apartment numbers
unique(apt_nums)

# Use regex to extract the first digit of these 
first_digits <- sub("^([0-9]).*", "\\1", apt_nums)

# Put these first digits in a table to get the frequency of them occurring in our dataset, and sort in decreasing order so that we see which are most common
sort(table(first_digits), decreasing = TRUE)

length(apt_nums) # total of 118 values in apt_nums
```

Benford's Law states that in sets that obey the law, the number 1 appears as the leading significant digit about 30% of the time, while 9 appears as the leading significant digit less than 5% of the time. However, in this data, 9 is the leading number 14.4% of the time (17 / 118), beating 1 which is sitting at 12.7% (15 / 118). Thus, since the apartment numbers do not appear to follow Benford’s law, I do not think the apartment numbers would pass as real data.

### Attribution of Sources:

-   1a: used chat gpt to figure out which library will let me read the .xpt files in R

-   1a: <https://www.datacamp.com/doc/r/merging> used this to figure out how to merge two datasets in R

-   1b: <https://r-coder.com/cut-r/> used this for the cut() function to bucket ages together

-   1b: <https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html> used this link for all my kable formatting

-   1c: <https://rpubs.com/snijesh/odds-ratio> used this to learn how to extract odds ratios/ what that means

-   1c: <https://www.rdocumentation.org/packages/pscl/versions/1.5.9/topics/pR2> used for psuedo r\^2 extraction

-   1c: <https://stackoverflow.com/questions/41200881/how-to-extract-aic-from-glm> used for AIC values, was not aware it was part of the built in summary

-   1d: <https://stackoverflow.com/questions/3872070/how-to-force-r-to-use-a-specified-factor-level-as-reference-in-a-regression> used for releveling to change/confirm reference category

-   1d: <http://www.sthda.com/english/wiki/chi-square-goodness-of-fit-test-in-r> used for chi-squared, setting up the code

-   3a-3d: Regex notes from professor, class notes
